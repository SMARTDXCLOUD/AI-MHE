# -*- coding: utf-8 -*-
"""Using Outcome-Based Meta Machine Learning Approach Towards Predicting Medical Student Status_6_3_1402.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BzNtx8YGE30I5JtTeEKSuLXMcVplPnvx
"""

!pip install catboost
!pip install shap

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import  matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
from sklearn.preprocessing import OneHotEncoder
import scipy
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix,RocCurveDisplay,roc_curve
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
address="/content/drive/MyDrive/predict_pre_score_mums/"
number_row=997
data=pd.read_csv(address+"CMPIE_1_3_1402.csv",sep=';',nrows=number_row)
print("check for null",data.isnull().sum())
print(data.columns)
data.info()
col_start=1
col_end=26

print("#######################################")
print(data["CMPIE Status"].value_counts())
data["GPA in Basic Science"].sort_values()
data['Normalized CMPIE MARK Binned'] = pd.qcut(data["Normlized CMPIE MARK "], q=10, duplicates='drop')
print(data["Normalized CMPIE MARK Binned"].value_counts())
print (data.info())
print(pd.crosstab(data["CMPIE Status"] ,data["CCA Status"]))
print("CMPIE Status",data["CMPIE Status"].value_counts())
print("CCA Status",data["CCA Status"].value_counts())

from sklearn.preprocessing import MinMaxScaler
# create a scaler object
scaler = MinMaxScaler(feature_range=(-1,1))
print("CCA Score",data["CCA Score"].value_counts())
print("CCA Status",data["CCA Score"].describe())
# fit and transform the "CCA Score" column using the scaler
normalized_data = scaler.fit_transform(data[["CCA Score"]])
# replace the "CCA Score" column with the normalized data
data["CCA Score"] = normalized_data
print("CCA Status",data["CCA Score"].describe())
print("Normlized CMPIE MARK ",data["Normlized CMPIE MARK "].describe())

# create a contingency table of columns 'A' and 'B'
data222=data
data222.dropna(subset=["CCA Status"], inplace=True)
data222.info()
cont_table = pd.crosstab(data222["CCA Status"], data222["CMPIE Status"])

# create a heatmap of the contingency table with formatted annotations
sns.heatmap(cont_table, annot=True,  fmt='.0f',cmap='coolwarm')



# add labels and a title to the plot
plt.xlabel('CCA Status')
plt.ylabel('CMPIE Status')

# display the plot
plt.show()

"""data statistic

---


"""

import catboost as cb
import pandas as pd
import shap

# Split the dataset into features and target
X = data.iloc[:, 6:21]
y = data["CMPIE Status"]

# Create an empty dictionary to store the categorical features
cat_features = {}

# Loop through the columns in the DataFrame
for col in X.columns:
    # Check if the column is categorical
    if X[col].dtype == 'object':
        # Store the unique values in the dictionary
        cat_features[col] = list(X[col].unique())


# Define the hyperparameters to search over
param_grid = {
    "iterations": [100, 200, 300],
    "learning_rate": [0.01, 0.05, 0.1],
    "depth": [4, 6, 8],
    "l2_leaf_reg": [1, 3, 5]
}

# Train a CatBoost classifier on the data
model = cb.CatBoostClassifier(random_seed=42)

# Set the feature names for the model
model.set_feature_names(list(X.columns))

# Perform a grid search over the hyperparameters
grid_search = GridSearchCV(model, param_grid,cv=5)

# Fit the grid search to the data
grid_search.fit(X, y, cat_features=list(cat_features.keys()))

# Print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

# Train a new CatBoost classifier using the best hyperparameters
best_model = cb.CatBoostClassifier(iterations=grid_search.best_params_['iterations'],
                                    learning_rate=grid_search.best_params_['learning_rate'],
                                    depth=grid_search.best_params_['depth'],
                                    l2_leaf_reg=grid_search.best_params_['l2_leaf_reg'],
                                    random_seed=42)

# Set the feature names for the model
best_model.set_feature_names(list(X.columns))

# Fit the model to the data
best_model.fit(X, y, cat_features=list(cat_features.keys()))


# Calculate the SHAP values for the dataset using the CatBoost method
explainer = shap.Explainer(best_model)
shap_values = explainer(X)

# Configure the appearance of the summary plot
shap.waterfall_plot(shap_values[0],                    show=False)

# Configure the appearance of the summary plot
#shap.summary_plot(shap_values,    X,                  plot_type="bar",                   color="dodgerblue",                   alpha=0.7,                  sort=True,                  show=False)

# Add a title to the plot
plt.title("SHAP Feature Importance Summary Plot")

# Add axis labels
plt.xlabel("SHAP Value")
plt.ylabel("Feature")

# Show the plot
plt.show()


plt.savefig(address+"Shap.png", dpi=300, bbox_inches="tight")

import seaborn as sns
import seaborn as sns
sns.set(font_scale=1.5)
custom_palette = sns.color_palette(["#006699", "#66CCFF", "#99CCFF", "#003366", "#336699"])
sns.set_palette(custom_palette)
plt.rcParams["figure.figsize"] = [20.00, 10]


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Compute the count of observations in each bin.
bin_counts = data['Normalized CMPIE MARK Binned'].value_counts().sort_index()

# Compute the 95% CI of the counts using the binomial distribution.
ci = 1.96 * np.sqrt(bin_counts * (1 - bin_counts / len(data)))

# Create a custom color palette with a blue to white gradient.
colors = sns.color_palette("Blues", len(bin_counts))
colors = colors[::-1]

# Set the figure size.
plt.figure(figsize=(20, 6))

# Create a bar plot of the binned data with the custom color palette.
ax = sns.barplot(x=bin_counts.index, y=bin_counts, capsize=0.1, errwidth=1.5, yerr=ci, palette=colors)

# Set the axis labels and title.
plt.xlabel('Bin')
plt.ylabel('Count')
plt.title('Count of Observations')

# Show the plot.
plt.show()

plt.savefig(address+"Normalized CMPIE MARK_distribution.png", dpi=300, bbox_inches="tight")

# Define custom bin labels.
labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']

# Use qcut to bin the data into 10 bins with approximately equal counts and add custom labels for each bin.
data['Normalized CMPIE MARK Binned'] = pd.qcut(data["Normlized CMPIE MARK "], q=10, duplicates='drop', labels=labels)
print(data['Normalized CMPIE MARK Binned'])
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming your continuous feature is in a column called "Normalized CMPIE MARK"
# and your custom bin labels are in a column called "Normalized CMPIE MARK Binned"
# You can change the column names to match your data.

# Create a histogram with custom bin labels.
#sns.displot(data=data, x="Normalized CMPIE MARK Binned", kde=True)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Compute the count of observations in each bin.
bin_counts = data['Normalized CMPIE MARK Binned'].value_counts().sort_index()

# Compute the 95% CI of the counts using the binomial distribution.
ci = 1.96 * np.sqrt(bin_counts * (1 - bin_counts / len(data)))

# Create a custom color palette with a blue to white gradient.
colors = sns.color_palette("Blues", len(bin_counts))
colors = colors[::-1]

# Set the figure size.
plt.figure(figsize=(20, 6))

# Create a bar plot of the binned data with the custom color palette.
ax = sns.barplot(x=bin_counts.index, y=bin_counts, capsize=0.1, errwidth=1.5, yerr=ci, palette=colors)

# Set the axis labels and title.
plt.xlabel('Bin')
plt.ylabel('Count')
plt.title('Count of Observations')

# Show the plot.
plt.show()

plt.savefig(address+"Normalized CMPIE MARK_distribution2.png", dpi=300, bbox_inches="tight")

categorical_features = ['Gender', 'Residency Status', 'Entrance Semester', 'Type of Admission', 'Internal Medicine', 'General Surgery', 'Pediatrics', 'Obstetrics and Gynecology', 'Pathology', 'Infectious Diseases', 'Neurosurgery', 'Dermatology', 'Orthopedic Surgery', 'Psychiatry', 'Pharmacology', 'Radiology', 'Urology', 'Ophthalmology', 'Otorhinolaryngology', 'Biostatistics and Epidemiology', 'Medical Ethics', 'GPA in Basic Science', 'GPA in Preclinical', 'GPA in clinical clerkship', 'Age at Entrance', 'Age at CMPIE', 'CMPIE Status', 'CCA Status', 'CMBSE Status ', 'Normalized CMPIE MARK Binned']

for feature in categorical_features:
    category_counts = data.groupby(['Normalized CMPIE MARK Binned', feature])[feature].count().unstack()
    category_counts = category_counts.fillna(0)
    print(f"Category counts for feature {feature}:")
    print(category_counts)

def plot_cluster_distributions(data, feature):
    category_counts = data.groupby(['Normalized CMPIE MARK Binned', feature])[feature].count().unstack()
    category_counts = category_counts.fillna(0)
    category_counts.plot(kind='bar', stacked=True)
    plt.title(f"{feature} Distribution per Cluster")
    plt.xlabel("Cluster")
    plt.ylabel("Count")
    plt.legend(title=feature)
    plt.show()

# Plot the categorical feature distributions for each cluster
for feature in categorical_features:
    plot_cluster_distributions(data, feature)

import seaborn as sns
import seaborn as sns
sns.set(font_scale=2.5)
custom_palette = sns.color_palette(["#006699", "#66CCFF", "#99CCFF", "#003366", "#336699"])
sns.set_palette(custom_palette)
plt.rcParams["figure.figsize"] = [20.00, 10]
plt.rcParams["figure.autolayout"] = True
sns.histplot(data=data,  x="Normlized CMPIE MARK ", hue="CMPIE Status", binwidth=0.05, kde=True)
plt.savefig(address+"histogram.png", dpi=300, bbox_inches="tight")

# Select categorical columns
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Select categorical columns
cat_cols = data.iloc[:, 1:33].select_dtypes(include=["object"]).columns.tolist()

# Set color palette to blue
custom_palette = sns.color_palette(["#006699", "#66CCFF", "#99CCFF", "#003366", "#336699"])
sns.set_palette(custom_palette)

# Create countplots for each categorical column and save as images
for col in cat_cols:
    plt.figure(figsize=(10,6))
    ax = sns.countplot(x=col, data=data)
    ax.set_ylabel("Count")
    ax.tick_params(axis="y")
    plt.title(f"Distribution of {col}")
    plt.savefig(address+"{col}_distribution.png", dpi=300, bbox_inches="tight")
    plt.show()

import seaborn as sns
sns.set(font_scale=2.5)
custom_palette = sns.color_palette(["#006699", "#66CCFF", "#99CCFF", "#003366", "#336699"])
sns.set_palette(custom_palette)
# Create a list of labels in the desired order
label_order = ['F', 'D', 'C','B','A','A+']
plt.rcParams["figure.figsize"] = [40.00, 10]
plt.rcParams["figure.autolayout"] = True
f, axes = plt.subplots(1, 4)
i=0
for column in data.columns[col_start:5]:
    #sns.catplot(data=data, x=column, hue="CMPIE Status" , kind="count",ax=axes[i])
    sns_plot=sns.countplot(data=data,x=data[column],  hue=data["CMPIE Status" ],ax=axes[i%4])
    #print(data[column])
    i=i+1
fig=sns_plot.get_figure()
fig.savefig(address+"statistic1-1.jpeg", dpi=300, bbox_inches="tight")
warnings.filterwarnings("ignore", category=FutureWarning)
plt.show()

plt.rcParams["figure.figsize"] = [40.00, 40]
plt.rcParams["figure.autolayout"] = True
f, axes = plt.subplots(4, 4)
i=0
for column in data.columns[6:22]:
    #sns.catplot(data=data, x=column, hue="CMPIE Status" , kind="count",ax=axes[i])
    sns_plot=sns.countplot(data=data,x=data[column], order=label_order, hue=data["CMPIE Status" ],ax=axes[i//4,i%4])
    #print(data[column])
    i=i+1
fig=sns_plot.get_figure()
fig.savefig(address+"statistic1-2.jpeg", dpi=300, bbox_inches="tight")
warnings.filterwarnings("ignore", category=FutureWarning)
plt.show()

plt.rcParams["figure.figsize"] = [20.00, 40]
plt.rcParams["figure.autolayout"] = True

f, axes = plt.subplots(13,2)
i=0

for column in data.columns[col_start:col_end+1]:
    #sns.catplot(data=data, x=column, hue="CMPIE Status" , kind="count",ax=axes[i])
    sns_plot1=sns.countplot(data=data,x=data[column], palette="coolwarm", hue=data["CMPIE Status" ], ax=axes[i//2,i%2])
   # print(data[column])
    i=i+1
fig1=sns_plot1.get_figure()
fig1.savefig(address+"statistic2.png", dpi=300, bbox_inches="tight")

from scipy.stats import chi2_contingency

import seaborn as sns
sns.set(font_scale=2)
custom_palette = sns.color_palette(["#006699", "#66CCFF", "#99CCFF", "#003366", "#336699"])
sns.set_palette(custom_palette)

plt.rcParams["figure.figsize"] = [20.00, 20]
plt.rcParams["figure.autolayout"] = True
# Create a matrix to store the Cramer's V values
corr_matrix = np.zeros((col_end, col_end))

# Calculate Cramer's V for all pairs of columns
for i in range(1, col_end+1):
    for j in range(i + 1, col_end+1):
        contingency_table = pd.crosstab(data.iloc[:, i], data.iloc[:, j])
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        n = contingency_table.sum().sum()
        phi2 = chi2 / n
        r, k = contingency_table.shape
        phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
        r_corr = r - ((r - 1) ** 2) / (n - 1)
        k_corr = k - ((k - 1) ** 2) / (n - 1)

        # Store the Cramer's V value in the correlation matrix
        corr_matrix[i - 1, j - 1] = np.sqrt(phi2corr / min((k_corr - 1), (r_corr - 1)))
        corr_matrix[j - 1, i - 1] = np.sqrt(phi2corr / min((k_corr - 1), (r_corr - 1)))

# Create a DataFrame from the correlation matrix
corr_df = pd.DataFrame(corr_matrix, columns=data.columns[col_start:col_end+1], index=data.columns[col_start:col_end+1])

# Create a mask to hide the upper triangle of the heatmap
mask = np.triu(np.ones_like(corr_df, dtype=np.bool))

# Draw a heatmap of the correlation matrix with values annotated in the cells
sns.heatmap(corr_df, cmap="Blues", mask=mask)

# Display the plot
plt.show()
plt.savefig(address+"heaatmap.png", dpi=300, bbox_inches="tight")
# Save the correlation matrix to a CSV file
corr_df.to_csv(address+"corr_matrix.csv")

# Set a threshold value for the correlation coefficient
threshold = 0.5

# Find the pairs of columns with correlation coefficient higher than the threshold
high_corr = np.where(corr_df > threshold)

# Print the pairs of columns with high correlation
for i, j in zip(*high_corr):
    print(f"{corr_df.columns[i]} and {corr_df.columns[j]} have a correlation coefficient of {corr_df.iloc[i, j]:.3f}")

"""t-test"""

import pandas as pd
from scipy.stats import chi2_contingency

import pandas as pd
from scipy.stats import chi2_contingency

# Select categorical columns
cat_cols = data.iloc[:, 1:28].select_dtypes(include=["object"]).columns.tolist()

# Initialize dictionary to store results
results = {}

# Loop over categorical columns and perform chi-square test
for col in cat_cols:
    contingency_table = pd.crosstab(data[col], data["CMPIE Status"])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    results[col] = {"Test Statistic": chi2, "p-value": p}

# Convert dictionary to DataFrame and format results
results_df = pd.DataFrame.from_dict(results, orient="index")
results_df["p-value"] = results_df["p-value"].round(3)
results_df["Test Statistic"] = results_df["Test Statistic"].round(3)

# Determine significant columns
sig_cols = results_df[results_df["p-value"] < 0.05].index.tolist()

# Displaythe results in a table
print("Chi-Square Test Results:")
print(results_df)

# Display significant columns
if sig_cols:
    print("\nSignificant Categorical Features:")
    print(sig_cols)
else:
    print("\nNo significant differences found.")
results_df.to_csv(address+"chi2.csv")

"""##### LET START MPDELING

ONE HOT ENCODDEEEERRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR
"""

##### 2 state
x_all_1=data.iloc[:,1:5]
x_all_2=data.iloc[:,6:27]
y_all=data["CMPIE Status"].values
one_hot_encoder=OneHotEncoder(drop="first") # two state one state drop
onehot_x1=one_hot_encoder.fit_transform(x_all_1).toarray()
newcol=one_hot_encoder.get_feature_names_out()
data_1=pd.DataFrame(onehot_x1,columns=newcol)
data_1.info()
one_hot_encoder=OneHotEncoder()
onehot_x1=one_hot_encoder.fit_transform(x_all_2).toarray()
newcol=one_hot_encoder.get_feature_names_out()
data_2=pd.DataFrame(onehot_x1,columns=newcol)
data_2.info()
one_hot_encoder=OneHotEncoder(drop="first")
onehot_xT=one_hot_encoder.fit_transform(y_all.reshape(-1,1)).toarray()
newcol=one_hot_encoder.get_feature_names_out()
data_T=pd.DataFrame(data=onehot_xT,columns=newcol)
result = pd.concat([data_1, data_2,data_T], axis=1)
for column in result.columns:
    result[column] = result[column].astype('int')
result.to_csv(address+'onhot_results.csv', index=False)
print(result.sum())
print(result.shape[0])
print(result.shape[1])
selected_cols = []
for col in result.columns:
    prop_true = result[col].sum()/result.shape[0]
    if prop_true < 0.01:
      print(f"Column '{col}' has less than 1% True values ({prop_true:.2%})")
      selected_cols.append(col)
print(selected_cols)

df_Xarray=result.iloc[:,:-1].values
df_Yarray=result.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(df_Xarray,df_Yarray,test_size=0.33, random_state=42,shuffle=True)
print(X_train.shape)
print(result.shape)

"""without smote"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,auc,roc_curve,RocCurveDisplay
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)
classes = classifier.classes_
y_pred = classifier.predict(X_test)

counts = np.bincount(y_test)

num_zeros = counts[0]
num_ones = counts[1]

total = len(y_test)
percent_zeros = num_zeros / total * 100
percent_ones = num_ones / total * 100

print("Percentage of 0s:", percent_zeros)
print("Percentage of 1s:", percent_ones)


from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
# X and y are the input features and labels, respectively
# model is the classification model to be trained
train_sizes, train_scores, test_scores = learning_curve(classifier, df_Xarray,df_Yarray, cv=5)
# Calculate mean and standard deviation of train and test scores across CV folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.plot(train_sizes, train_mean, label='Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.plot(train_sizes, test_mean, label='Validation')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.savefig(address+"auc_learning_curve_without_smote.png", dpi=300, bbox_inches="tight")

plt.rcParams["figure.figsize"] = [12.00, 6] ## سن امتحان حذف درس اخلاق
plt.rcParams["figure.autolayout"] = True
fpr, tpr, _ = roc_curve(y_test, y_pred)
AUC=auc(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt="d",cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
plt.savefig(address+"confusion_without_smote.png", dpi=300, bbox_inches="tight")



plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % AUC)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for outer')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.legend(loc="lower right")
plt.show()
plt.savefig(address+"auc_without_smote.png", dpi=300, bbox_inches="tight")

Accuracy = accuracy_score(y_test, y_pred) * 100
FSCORE=f1_score(y_test, y_pred, average=None)
RECALL=recall_score(y_test, y_pred, average=None)
PRECISION=precision_score(y_test, y_pred, average=None)
for i in range(len(classes)):
      print("FSCOR", classes[i], ": ",FSCORE[i])
      print("RECALL", classes[i], ": ",RECALL[i])
      print("PRECISION", classes[i], ": ",PRECISION[i])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# True labels and predicted labels for test set
y_true_train = y_train
y_pred_train = classifier.predict(X_train)
    # True labels and predicted labels for train set
y_true_test = y_test
y_pred_test = classifier.predict(X_test)# predicted labels for train set
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for test set
accuracy_test = accuracy_score(y_true_test, y_pred_test)
precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_score_test = f1_score(y_true_test, y_pred_test)
tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true_test, y_pred_test).ravel()
specificity_test = tn_test / (tn_test + fp_test)
sensitivity_test = tp_test / (tp_test + fn_test)
auc_test = roc_auc_score(y_true_test, y_pred_test)
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for train set
accuracy_train = accuracy_score(y_true_train, y_pred_train)
precision_train = precision_score(y_true_train, y_pred_train)
recall_train = recall_score(y_true_train, y_pred_train)
f1_score_train = f1_score(y_true_train, y_pred_train)
tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_true_train, y_pred_train).ravel()
specificity_train = tn_train / (tn_train + fp_train)
sensitivity_train = tp_train / (tp_train + fn_train)
auc_train = roc_auc_score(y_true_train, y_pred_train)
# Print the evaluation metrics
print("Test set metrics:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1 score:", f1_score_test)
print("Specificity:", specificity_test)
print("sensitivity:", sensitivity_test)
print("AUC:", auc_test)
print("Train set metrics:")
print("Accuracy:", accuracy_train)
print("Precision:", precision_train)
print("Recall:", recall_train)
print("F1 score:", f1_score_train)
print("Specificity:", specificity_train)
print("sensitivity:", sensitivity_train)
print("AUC:", auc_train)

# Check for overfitting by comparing metrics of test and train sets
#One common rule of thumb is that the difference in performance metrics between the test and train sets should be less than 5%.
print("Overfitting check (between the test and train sets should be less than 5%):")
print("Accuracy difference:", np.abs(accuracy_test - accuracy_train))
print("Precision difference:", np.abs(precision_test - precision_train))
print("Recall difference:", np.abs(recall_test - recall_train))
print("F1 score difference:", np.abs(f1_score_test - f1_score_train))
print("Specificity difference:", np.abs(specificity_test - specificity_train))
print("Sensitivity difference:", abs(sensitivity_test - sensitivity_train))
print("AUC difference:", np.abs(auc_test - auc_train))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
# y_true is the true labels for the test set, and y_probs is the predicted probabilities
y_true = y_test# true labels for test set
y_probs = y_pred # predicted probabilities for test set
# Calculate calibration curve
frac_pos, mean_pred = calibration_curve(y_true, y_probs, n_bins=10)
# Plot calibration curve
plt.plot(mean_pred, frac_pos, marker='o', linewidth=1, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positive samples')
plt.legend()
plt.show()
plt.savefig(address+"calibration curve_stacking_without_smote.png", dpi=300, bbox_inches="tight")

"""with smote"""

#from imblearn.over_sampling import BorderlineSMOTE
from imblearn.over_sampling import SMOTEN
#from imblearn.over_sampling import KMeansSMOTE
#from imblearn.over_sampling import SMOTE, ADASYN
#from imblearn.combine import SMOTETomek
#from imblearn.over_sampling import SMOTE, ADASYN

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,auc,roc_curve,RocCurveDisplay
from sklearn.linear_model import LogisticRegression

from imblearn.combine import SMOTEENN
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.feature_selection import mutual_info_classif

resamble = SMOTEENN(random_state=0)
X_r,Y_r=resamble.fit_resample(df_Xarray,df_Yarray)
c1=result.columns[1:]
df_os=pd.DataFrame(X_r,columns=c1)
df_os["target"]=Y_r
counter_Aresamble = df_os.iloc[:,-1].value_counts()
print("\n")
print("counter_aresamble :  ",counter_Aresamble)
df_Xarray=df_os.iloc[:,:-1].values
df_Yarray=df_os.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(df_Xarray,df_Yarray,test_size=0.33, random_state=42,shuffle=True)
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)
classes = classifier.classes_
y_pred = classifier.predict(X_test)


# Compute the mutual information between each feature and the target variable
mi = mutual_info_classif(X_train, y_train)
# Sort the feature importances in descending order
sorted_idx = np.argsort(mi)[::-1]
sorted_names = c1[sorted_idx]
sorted_mi = mi[sorted_idx]
# Create a bar chart of the sorted feature importances
fig, ax = plt.subplots()
ax.bar(sorted_names, sorted_mi)
fig, ax = plt.subplots(figsize=(20, 6))
ax.bar(sorted_names, sorted_mi)
ax.set_xticklabels(sorted_names, rotation=90, fontsize=12)
ax.set_ylabel('Mutual Information', fontsize=12)
ax.set_title('Feature Importances', fontsize=16)
plt.show()
plt.savefig(address+"Feature Importances.png", dpi=300, bbox_inches="tight")

counts = np.bincount(y_test)

num_zeros = counts[0]
num_ones = counts[1]

total = len(y_test)
percent_zeros = num_zeros / total * 100
percent_ones = num_ones / total * 100

print("Percentage of 0s:", percent_zeros)
print("Percentage of 1s:", percent_ones)


from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
# X and y are the input features and labels, respectively
# model is the classification model to be trained
train_sizes, train_scores, test_scores = learning_curve(classifier, df_Xarray,df_Yarray, cv=5)
# Calculate mean and standard deviation of train and test scores across CV folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.plot(train_sizes, train_mean, label='Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.plot(train_sizes, test_mean, label='Validation')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.savefig(address+"auc_learning_curve_smote.png", dpi=300, bbox_inches="tight")

plt.rcParams["figure.figsize"] = [12.00, 6] ## سن امتحان حذف درس اخلاق
plt.rcParams["figure.autolayout"] = True
fpr, tpr, _ = roc_curve(y_test, y_pred)
AUC=auc(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt="d",cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
plt.savefig(address+"confusion_smote.png", dpi=300, bbox_inches="tight")



plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % AUC)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for outer')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.legend(loc="lower right")
plt.show()
plt.savefig(address+"auc_out_smote.png", dpi=300, bbox_inches="tight")

Accuracy = accuracy_score(y_test, y_pred) * 100
FSCORE=f1_score(y_test, y_pred, average=None)
RECALL=recall_score(y_test, y_pred, average=None)
PRECISION=precision_score(y_test, y_pred, average=None)
for i in range(len(classes)):
      print("FSCOR", classes[i], ": ",FSCORE[i])
      print("RECALL", classes[i], ": ",RECALL[i])
      print("PRECISION", classes[i], ": ",PRECISION[i])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# True labels and predicted labels for test set
y_true_train = y_train
y_pred_train = classifier.predict(X_train)
    # True labels and predicted labels for train set
y_true_test = y_test
y_pred_test = classifier.predict(X_test)# predicted labels for train set
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for test set
accuracy_test = accuracy_score(y_true_test, y_pred_test)
precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_score_test = f1_score(y_true_test, y_pred_test)
tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true_test, y_pred_test).ravel()
specificity_test = tn_test / (tn_test + fp_test)
sensitivity_test = tp_test / (tp_test + fn_test)
auc_test = roc_auc_score(y_true_test, y_pred_test)
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for train set
accuracy_train = accuracy_score(y_true_train, y_pred_train)
precision_train = precision_score(y_true_train, y_pred_train)
recall_train = recall_score(y_true_train, y_pred_train)
f1_score_train = f1_score(y_true_train, y_pred_train)
tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_true_train, y_pred_train).ravel()
specificity_train = tn_train / (tn_train + fp_train)
sensitivity_train = tp_train / (tp_train + fn_train)
auc_train = roc_auc_score(y_true_train, y_pred_train)
# Print the evaluation metrics
print("Test set metrics:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1 score:", f1_score_test)
print("Specificity:", specificity_test)
print("sensitivity:", sensitivity_test)
print("AUC:", auc_test)
print("Train set metrics:")
print("Accuracy:", accuracy_train)
print("Precision:", precision_train)
print("Recall:", recall_train)
print("F1 score:", f1_score_train)
print("Specificity:", specificity_train)
print("sensitivity:", sensitivity_train)
print("AUC:", auc_train)

# Check for overfitting by comparing metrics of test and train sets
#One common rule of thumb is that the difference in performance metrics between the test and train sets should be less than 5%.
print("Overfitting check (between the test and train sets should be less than 5%):")
print("Accuracy difference:", np.abs(accuracy_test - accuracy_train))
print("Precision difference:", np.abs(precision_test - precision_train))
print("Recall difference:", np.abs(recall_test - recall_train))
print("F1 score difference:", np.abs(f1_score_test - f1_score_train))
print("Specificity difference:", np.abs(specificity_test - specificity_train))
print("Sensitivity difference:", abs(sensitivity_test - sensitivity_train))
print("AUC difference:", np.abs(auc_test - auc_train))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
# y_true is the true labels for the test set, and y_probs is the predicted probabilities
y_true = y_test# true labels for test set
y_probs = y_pred # predicted probabilities for test set
# Calculate calibration curve
frac_pos, mean_pred = calibration_curve(y_true, y_probs, n_bins=10)
# Plot calibration curve
plt.plot(mean_pred, frac_pos, marker='o', linewidth=1, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positive samples')
plt.legend()
plt.show()
plt.savefig(address+"calibration curve_stacking_smote.png", dpi=300, bbox_inches="tight")

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.ensemble import StackingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression


# Define the base models
models = [
    ('rf', RandomForestClassifier(random_state=42, max_depth=5,min_samples_leaf=5,n_estimators=100)),
    #('gb', GradientBoostingClassifier(random_state=42, max_depth=5)),
      ('xgb', XGBClassifier(random_state=42, max_depth=5,n_estimators=100)),
      ('ADA',AdaBoostClassifier(algorithm= 'SAMME',learning_rate= 0.1,n_estimators=100))
]

# Define the meta-model
meta_model = LogisticRegression(random_state=42)

# Define the stacking model
stacking_model = make_pipeline(
    StandardScaler(),
    StackingClassifier(
        estimators=models,
        final_estimator=meta_model,
        cv=KFold(n_splits=5, shuffle=True, random_state=42),
        stack_method='predict_proba',
    )
)
#---------------------------------------------------------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(df_Xarray,df_Yarray,test_size=0.33, random_state=42,shuffle=True)
#stacking_scores = cross_val_score(stacking_model, X_train, y_train, cv=5)
#----------------------------------------------------------------------------------------------------------------
# Load the dataset
X, y = df_Xarray, df_Yarray

# Define the outer cross-validation loop
outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)

# Define the inner cross-validation loop
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Define the metrics to evaluate
scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']

# Evaluate the stacking model using nested cross-validation
stacking_scores = cross_validate(stacking_model, X, y, cv=outer_cv, scoring=scoring)

# Print the results
print('Stacking model:')
for metric in scoring:
    scores = stacking_scores[f'test_{metric}']
    print(f'{metric}: {scores.mean():.3f} +/- {2*scores.std():.3f}')

# Evaluate the base models using cross-validation (train)
base_scores = []
for name, model in models:
    base_scores = cross_validate(model, X, y, cv=outer_cv, scoring=scoring)
    print(name)
    for metric in scoring:
        scores = base_scores[f'test_{metric}']
        print(f'{metric}: {scores.mean():.3f} +/- {2*scores.std():.3f}')

#------------------------------------------------------------------------------------------------------------------------------

classifier = stacking_model
classifier.fit(X_train, y_train)
classes = classifier.classes_
y_pred = classifier.predict(X_test)
score = accuracy_score(y_test, y_pred)
print(f'Stacking model: {score.mean():.3f}')

#-----------------------------------------------------------


#-----------------------------------------------------------

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
# X and y are the input features and labels, respectively
# model is the classification model to be trained
train_sizes, train_scores, test_scores = learning_curve(classifier, df_Xarray,df_Yarray, cv=5)
# Calculate mean and standard deviation of train and test scores across CV folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.plot(train_sizes, train_mean, label='Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.plot(train_sizes, test_mean, label='Validation')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.savefig(address+"auc_learning_curve_smote_stacking.png", dpi=300, bbox_inches="tight")

plt.rcParams["figure.figsize"] = [12.00, 6] ## سن امتحان حذف درس اخلاق
plt.rcParams["figure.autolayout"] = True
fpr, tpr, _ = roc_curve(y_test, y_pred)
from sklearn import metrics
AUC=metrics.auc(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt="d",cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
plt.savefig(address+"confusion_smote_stacking.png", dpi=300, bbox_inches="tight")


plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % AUC)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for outer')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.legend(loc="lower right")
plt.show()
plt.savefig(address+"auc__smote_stacking.png", dpi=300, bbox_inches="tight")

Accuracy = accuracy_score(y_test, y_pred) * 100
FSCORE=f1_score(y_test, y_pred, average=None)
RECALL=recall_score(y_test, y_pred, average=None)
PRECISION=precision_score(y_test, y_pred, average=None)
for i in range(len(classes)):
      print("FSCOR", classes[i], ": ",FSCORE[i])
      print("RECALL", classes[i], ": ",RECALL[i])
      print("PRECISION", classes[i], ": ",PRECISION[i])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# True labels and predicted labels for test set
y_true_train = y_train
y_pred_train = classifier.predict(X_train)
    # True labels and predicted labels for train set
y_true_test = y_test
y_pred_test = classifier.predict(X_test)# predicted labels for train set
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for test set
accuracy_test = accuracy_score(y_true_test, y_pred_test)
precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_score_test = f1_score(y_true_test, y_pred_test)
tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true_test, y_pred_test).ravel()
specificity_test = tn_test / (tn_test + fp_test)
sensitivity_test = tp_test / (tp_test + fn_test)
auc_test = roc_auc_score(y_true_test, y_pred_test)
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for train set
accuracy_train = accuracy_score(y_true_train, y_pred_train)
precision_train = precision_score(y_true_train, y_pred_train)
recall_train = recall_score(y_true_train, y_pred_train)
f1_score_train = f1_score(y_true_train, y_pred_train)
tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_true_train, y_pred_train).ravel()
specificity_train = tn_train / (tn_train + fp_train)
sensitivity_train = tp_train / (tp_train + fn_train)
auc_train = roc_auc_score(y_true_train, y_pred_train)
# Print the evaluation metrics
print("Test set metrics:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1 score:", f1_score_test)
print("Specificity:", specificity_test)
print("sensitivity:", sensitivity_test)
print("AUC:", auc_test)

print("Train set metrics:")
print("Accuracy:", accuracy_train)
print("Precision:", precision_train)
print("Recall:", recall_train)
print("F1 score:", f1_score_train)
print("Specificity:", specificity_train)
print("sensitivity:", sensitivity_train)
print("AUC:", auc_train)

# Check for overfitting by comparing metrics of test and train sets
#One common rule of thumb is that the difference in performance metrics between the test and train sets should be less than 5%.
print("Overfitting check (between the test and train sets should be less than 5%):")
print("Accuracy difference:", np.abs(accuracy_test - accuracy_train))
print("Precision difference:", np.abs(precision_test - precision_train))
print("Recall difference:", np.abs(recall_test - recall_train))
print("F1 score difference:", np.abs(f1_score_test - f1_score_train))
print("Specificity difference:", np.abs(specificity_test - specificity_train))
print("Sensitivity difference:", abs(sensitivity_test - sensitivity_train))
print("AUC difference:", np.abs(auc_test - auc_train))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
# y_true is the true labels for the test set, and y_probs is the predicted probabilities
y_true = y_test# true labels for test set
y_probs = y_pred # predicted probabilities for test set
# Calculate calibration curve
frac_pos, mean_pred = calibration_curve(y_true, y_probs, n_bins=10)
# Plot calibration curve
plt.plot(mean_pred, frac_pos, marker='o', linewidth=1, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positive samples')
plt.legend()
plt.show()
plt.savefig(address+"calibration curve_smote_stacking.png", dpi=300, bbox_inches="tight")

import pickle

# Train your model
# ...

# Save your trained model to a file
with open(address+"final_model.pkl", 'wb') as f:
    pickle.dump(classifier, f)

print (result.columns)
df_Xarray2=result.iloc[:,:-1].values
df_Yarray2=result.iloc[:,-1].values
y_pred_test = classifier.predict(df_Xarray2)
print(y_pred_test)
y_pred_test = y_pred_test.reshape(-1, 1)
cols_except_last = result.iloc[:, :-1].columns.tolist()

# Predict on the test data

data_3=pd.DataFrame(df_Xarray2,columns=cols_except_last)
y_pred_test=pd.DataFrame(data=y_pred_test,columns=["predicted_CMPIE_status"])
cca_status=pd.DataFrame(data=data,columns=["CCA Status"])
cca_status=cca_status.replace({'PASS': 1, 'FAIL': 0})
result2 = pd.concat([data_3, y_pred_test,cca_status], axis=1)
result2 = result2.dropna(subset=["CCA Status"])
result2["CCA Status"]=result2["CCA Status"].astype(int)
print(result2)
result2.to_csv(address+'outcome_based_model_input.csv', index=False)

null_counts = result2.isnull().sum()

# print the number of null values in each column
print(null_counts)

result2= pd.read_csv(address+'outcome_based_model_input.csv')
df_Xarray3=result2.iloc[:,:-1].values
df_Yarray3=result2.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(df_Xarray3,df_Yarray3,test_size=0.33, random_state=42,shuffle=True)
print(X_train.shape)
print(result2.shape)
result2.columns

#from imblearn.over_sampling import BorderlineSMOTE
from imblearn.over_sampling import SMOTEN
#from imblearn.over_sampling import KMeansSMOTE
#from imblearn.over_sampling import SMOTE, ADASYN
#from imblearn.combine import SMOTETomek
#from imblearn.over_sampling import SMOTE, ADASYN

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,auc,roc_curve,RocCurveDisplay
from sklearn.linear_model import LogisticRegression

from imblearn.combine import SMOTEENN
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.feature_selection import mutual_info_classif

resamble = SMOTEENN(random_state=0)
X_r,Y_r=resamble.fit_resample(df_Xarray3,df_Yarray3)
c1=result.columns[:]
df_os=pd.DataFrame(X_r,columns=c1)
df_os["target"]=Y_r
counter_Aresamble = df_os.iloc[:,-1].value_counts()
print("\n")
print("counter_aresamble :  ",counter_Aresamble)
df_Xarray3=df_os.iloc[:,:-1].values
df_Yarray3=df_os.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(df_Xarray3,df_Yarray3,test_size=0.33, random_state=42,shuffle=True)
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)
classes = classifier.classes_
y_pred = classifier.predict(X_test)


# Compute the mutual information between each feature and the target variable
mi = mutual_info_classif(X_train, y_train)
# Sort the feature importances in descending order
sorted_idx = np.argsort(mi)[::-1]
sorted_names = c1[sorted_idx]
sorted_mi = mi[sorted_idx]
# Create a bar chart of the sorted feature importances
fig, ax = plt.subplots()
ax.bar(sorted_names, sorted_mi)
fig, ax = plt.subplots(figsize=(20, 6))
ax.bar(sorted_names, sorted_mi)
ax.set_xticklabels(sorted_names, rotation=90, fontsize=12)
ax.set_ylabel('Mutual Information', fontsize=12)
ax.set_title('Feature Importances', fontsize=16)
plt.show()
plt.savefig(address+"Feature Importances.png", dpi=300, bbox_inches="tight")

counts = np.bincount(y_test)

num_zeros = counts[0]
num_ones = counts[1]

total = len(y_test)
percent_zeros = num_zeros / total * 100
percent_ones = num_ones / total * 100

print("Percentage of 0s:", percent_zeros)
print("Percentage of 1s:", percent_ones)


from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
# X and y are the input features and labels, respectively
# model is the classification model to be trained
train_sizes, train_scores, test_scores = learning_curve(classifier, df_Xarray3,df_Yarray3, cv=5)
# Calculate mean and standard deviation of train and test scores across CV folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.plot(train_sizes, train_mean, label='Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.plot(train_sizes, test_mean, label='Validation')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.savefig(address+"auc_learning_curve_smote.png", dpi=300, bbox_inches="tight")

plt.rcParams["figure.figsize"] = [12.00, 6] ## سن امتحان حذف درس اخلاق
plt.rcParams["figure.autolayout"] = True
fpr, tpr, _ = roc_curve(y_test, y_pred)
AUC=auc(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt="d",cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
plt.savefig(address+"confusion_smote.png", dpi=300, bbox_inches="tight")



plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % AUC)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for outer')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.legend(loc="lower right")
plt.show()
plt.savefig(address+"auc_out_smote.png", dpi=300, bbox_inches="tight")

Accuracy = accuracy_score(y_test, y_pred) * 100
FSCORE=f1_score(y_test, y_pred, average=None)
RECALL=recall_score(y_test, y_pred, average=None)
PRECISION=precision_score(y_test, y_pred, average=None)
for i in range(len(classes)):
      print("FSCOR", classes[i], ": ",FSCORE[i])
      print("RECALL", classes[i], ": ",RECALL[i])
      print("PRECISION", classes[i], ": ",PRECISION[i])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# True labels and predicted labels for test set
y_true_train = y_train
y_pred_train = classifier.predict(X_train)
    # True labels and predicted labels for train set
y_true_test = y_test
y_pred_test = classifier.predict(X_test)# predicted labels for train set
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for test set
accuracy_test = accuracy_score(y_true_test, y_pred_test)
precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_score_test = f1_score(y_true_test, y_pred_test)
tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true_test, y_pred_test).ravel()
specificity_test = tn_test / (tn_test + fp_test)
sensitivity_test = tp_test / (tp_test + fn_test)
auc_test = roc_auc_score(y_true_test, y_pred_test)
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for train set
accuracy_train = accuracy_score(y_true_train, y_pred_train)
precision_train = precision_score(y_true_train, y_pred_train)
recall_train = recall_score(y_true_train, y_pred_train)
f1_score_train = f1_score(y_true_train, y_pred_train)
tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_true_train, y_pred_train).ravel()
specificity_train = tn_train / (tn_train + fp_train)
sensitivity_train = tp_train / (tp_train + fn_train)
auc_train = roc_auc_score(y_true_train, y_pred_train)
# Print the evaluation metrics
print("Test set metrics:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1 score:", f1_score_test)
print("Specificity:", specificity_test)
print("sensitivity:", sensitivity_test)
print("AUC:", auc_test)
print("Train set metrics:")
print("Accuracy:", accuracy_train)
print("Precision:", precision_train)
print("Recall:", recall_train)
print("F1 score:", f1_score_train)
print("Specificity:", specificity_train)
print("sensitivity:", sensitivity_train)
print("AUC:", auc_train)

# Check for overfitting by comparing metrics of test and train sets
#One common rule of thumb is that the difference in performance metrics between the test and train sets should be less than 5%.
print("Overfitting check (between the test and train sets should be less than 5%):")
print("Accuracy difference:", np.abs(accuracy_test - accuracy_train))
print("Precision difference:", np.abs(precision_test - precision_train))
print("Recall difference:", np.abs(recall_test - recall_train))
print("F1 score difference:", np.abs(f1_score_test - f1_score_train))
print("Specificity difference:", np.abs(specificity_test - specificity_train))
print("Sensitivity difference:", abs(sensitivity_test - sensitivity_train))
print("AUC difference:", np.abs(auc_test - auc_train))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
# y_true is the true labels for the test set, and y_probs is the predicted probabilities
y_true = y_test# true labels for test set
y_probs = y_pred # predicted probabilities for test set
# Calculate calibration curve
frac_pos, mean_pred = calibration_curve(y_true, y_probs, n_bins=10)
# Plot calibration curve
plt.plot(mean_pred, frac_pos, marker='o', linewidth=1, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positive samples')
plt.legend()
plt.show()
plt.savefig(address+"calibration curve_stacking_smote.png", dpi=300, bbox_inches="tight")

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.ensemble import StackingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression


# Define the base models
models = [
    ('rf', RandomForestClassifier(random_state=42, max_depth=5,min_samples_leaf=5,n_estimators=100)),
    #('gb', GradientBoostingClassifier(random_state=42, max_depth=5)),
      ('xgb', XGBClassifier(random_state=42, max_depth=5,n_estimators=100)),
      ('ADA',AdaBoostClassifier(algorithm= 'SAMME',learning_rate= 0.1,n_estimators=100))
]

# Define the meta-model
meta_model = LogisticRegression(random_state=42)

# Define the stacking model
stacking_model = make_pipeline(
    StandardScaler(),
    StackingClassifier(
        estimators=models,
        final_estimator=meta_model,
        cv=KFold(n_splits=5, shuffle=True, random_state=42),
        stack_method='predict_proba',
    )
)
#---------------------------------------------------------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(df_Xarray3,df_Yarray3,test_size=0.33, random_state=42,shuffle=True)
#stacking_scores = cross_val_score(stacking_model, X_train, y_train, cv=5)
#----------------------------------------------------------------------------------------------------------------
# Load the dataset
X, y = df_Xarray3, df_Yarray3

# Define the outer cross-validation loop
outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)

# Define the inner cross-validation loop
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Define the metrics to evaluate
scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']

# Evaluate the stacking model using nested cross-validation
stacking_scores = cross_validate(stacking_model, X, y, cv=outer_cv, scoring=scoring)

# Print the results
print('Stacking model:')
for metric in scoring:
    scores = stacking_scores[f'test_{metric}']
    print(f'{metric}: {scores.mean():.3f} +/- {2*scores.std():.3f}')

# Evaluate the base models using cross-validation (train)
base_scores = []
for name, model in models:
    base_scores = cross_validate(model, X, y, cv=outer_cv, scoring=scoring)
    print(name)
    for metric in scoring:
        scores = base_scores[f'test_{metric}']
        print(f'{metric}: {scores.mean():.3f} +/- {2*scores.std():.3f}')

#------------------------------------------------------------------------------------------------------------------------------

classifier = stacking_model
classifier.fit(X_train, y_train)
classes = classifier.classes_
y_pred = classifier.predict(X_test)
score = accuracy_score(y_test, y_pred)
print(f'Stacking model: {score.mean():.3f}')

#-----------------------------------------------------------


#-----------------------------------------------------------

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
# X and y are the input features and labels, respectively
# model is the classification model to be trained
train_sizes, train_scores, test_scores = learning_curve(classifier, df_Xarray3,df_Yarray3, cv=5)
# Calculate mean and standard deviation of train and test scores across CV folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.plot(train_sizes, train_mean, label='Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.plot(train_sizes, test_mean, label='Validation')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.savefig(address+"auc_learning_curve_smote_stacking.png", dpi=300, bbox_inches="tight")

plt.rcParams["figure.figsize"] = [12.00, 6] ## سن امتحان حذف درس اخلاق
plt.rcParams["figure.autolayout"] = True
fpr, tpr, _ = roc_curve(y_test, y_pred)
from sklearn import metrics
AUC=metrics.auc(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt="d",cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
plt.savefig(address+"confusion_smote_stacking.png", dpi=300, bbox_inches="tight")


plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % AUC)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for outer')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.legend(loc="lower right")
plt.show()
plt.savefig(address+"auc__smote_stacking.png", dpi=300, bbox_inches="tight")

Accuracy = accuracy_score(y_test, y_pred) * 100
FSCORE=f1_score(y_test, y_pred, average=None)
RECALL=recall_score(y_test, y_pred, average=None)
PRECISION=precision_score(y_test, y_pred, average=None)
for i in range(len(classes)):
      print("FSCOR", classes[i], ": ",FSCORE[i])
      print("RECALL", classes[i], ": ",RECALL[i])
      print("PRECISION", classes[i], ": ",PRECISION[i])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# True labels and predicted labels for test set
y_true_train = y_train
y_pred_train = classifier.predict(X_train)
    # True labels and predicted labels for train set
y_true_test = y_test
y_pred_test = classifier.predict(X_test)# predicted labels for train set
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for test set
accuracy_test = accuracy_score(y_true_test, y_pred_test)
precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_score_test = f1_score(y_true_test, y_pred_test)
tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true_test, y_pred_test).ravel()
specificity_test = tn_test / (tn_test + fp_test)
sensitivity_test = tp_test / (tp_test + fn_test)
auc_test = roc_auc_score(y_true_test, y_pred_test)
    # Calculate accuracy, precision, recall, F1 score, specificity, and AUC for train set
accuracy_train = accuracy_score(y_true_train, y_pred_train)
precision_train = precision_score(y_true_train, y_pred_train)
recall_train = recall_score(y_true_train, y_pred_train)
f1_score_train = f1_score(y_true_train, y_pred_train)
tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_true_train, y_pred_train).ravel()
specificity_train = tn_train / (tn_train + fp_train)
sensitivity_train = tp_train / (tp_train + fn_train)
auc_train = roc_auc_score(y_true_train, y_pred_train)
# Print the evaluation metrics
print("Test set metrics:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1 score:", f1_score_test)
print("Specificity:", specificity_test)
print("sensitivity:", sensitivity_test)
print("AUC:", auc_test)

print("Train set metrics:")
print("Accuracy:", accuracy_train)
print("Precision:", precision_train)
print("Recall:", recall_train)
print("F1 score:", f1_score_train)
print("Specificity:", specificity_train)
print("sensitivity:", sensitivity_train)
print("AUC:", auc_train)

# Check for overfitting by comparing metrics of test and train sets
#One common rule of thumb is that the difference in performance metrics between the test and train sets should be less than 5%.
print("Overfitting check (between the test and train sets should be less than 5%):")
print("Accuracy difference:", np.abs(accuracy_test - accuracy_train))
print("Precision difference:", np.abs(precision_test - precision_train))
print("Recall difference:", np.abs(recall_test - recall_train))
print("F1 score difference:", np.abs(f1_score_test - f1_score_train))
print("Specificity difference:", np.abs(specificity_test - specificity_train))
print("Sensitivity difference:", abs(sensitivity_test - sensitivity_train))
print("AUC difference:", np.abs(auc_test - auc_train))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
# y_true is the true labels for the test set, and y_probs is the predicted probabilities
y_true = y_test# true labels for test set
y_probs = y_pred # predicted probabilities for test set
# Calculate calibration curve
frac_pos, mean_pred = calibration_curve(y_true, y_probs, n_bins=10)
# Plot calibration curve
plt.plot(mean_pred, frac_pos, marker='o', linewidth=1, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positive samples')
plt.legend()
plt.show()
plt.savefig(address+"calibration curve_smote_stacking.png", dpi=300, bbox_inches="tight")

# create a contingency table of columns 'A' and 'B'
data222=data
data222.dropna(subset=["CCA Status"], inplace=True)
data222.info()
cont_table = pd.crosstab(data222["CCA Status"], data222["CMPIE Status"], normalize='all')

# create a heatmap of the contingency table with formatted annotations
sns.heatmap(cont_table, annot=True,  fmt='.2f',cmap='coolwarm')



# add labels and a title to the plot
plt.xlabel('CCA Status')
plt.ylabel('CMPIE Status')

# display the plot
plt.show()